INFO : 2019-08-31 18:03:03,365 [run-main-0] *** Main args(0): 30.0 : args(1): 999
INFO : 2019-08-31 18:03:04,904 [run-main-0] *** areaOfInterestRadiusInKilometers: 30.0
INFO : 2019-08-31 18:03:04,905 [run-main-0] *** hitDaysHence: 1480975384904
INFO : 2019-08-31 18:03:04,971 [run-main-0] ***Spark Event Log directory ( /tmp/spark-events ) exists or was created: true
INFO : 2019-08-31 18:03:05,086 [run-main-0] Running Spark version 2.4.3
WARN : 2019-08-31 18:03:05,338 [run-main-0] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO : 2019-08-31 18:03:05,432 [run-main-0] Submitted application: aoi
INFO : 2019-08-31 18:03:05,482 [run-main-0] Changing view acls to: objektwerks
INFO : 2019-08-31 18:03:05,482 [run-main-0] Changing modify acls to: objektwerks
INFO : 2019-08-31 18:03:05,482 [run-main-0] Changing view acls groups to: 
INFO : 2019-08-31 18:03:05,483 [run-main-0] Changing modify acls groups to: 
INFO : 2019-08-31 18:03:05,483 [run-main-0] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(objektwerks); groups with view permissions: Set(); users  with modify permissions: Set(objektwerks); groups with modify permissions: Set()
INFO : 2019-08-31 18:03:05,785 [run-main-0] Successfully started service 'sparkDriver' on port 63769.
INFO : 2019-08-31 18:03:05,806 [run-main-0] Registering MapOutputTracker
INFO : 2019-08-31 18:03:05,821 [run-main-0] Registering BlockManagerMaster
INFO : 2019-08-31 18:03:05,824 [run-main-0] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
INFO : 2019-08-31 18:03:05,824 [run-main-0] BlockManagerMasterEndpoint up
INFO : 2019-08-31 18:03:05,833 [run-main-0] Created local directory at /private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/blockmgr-1b344086-1959-4142-9999-36ee2b9452b1
INFO : 2019-08-31 18:03:05,846 [run-main-0] MemoryStore started with capacity 2004.6 MB
INFO : 2019-08-31 18:03:05,858 [run-main-0] Registering OutputCommitCoordinator
INFO : 2019-08-31 18:03:05,923 [run-main-0] Logging initialized @12475ms
INFO : 2019-08-31 18:03:05,981 [run-main-0] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
INFO : 2019-08-31 18:03:05,999 [run-main-0] Started @12553ms
INFO : 2019-08-31 18:03:06,021 [run-main-0] Started ServerConnector@7abc100c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
INFO : 2019-08-31 18:03:06,021 [run-main-0] Successfully started service 'SparkUI' on port 4040.
INFO : 2019-08-31 18:03:06,050 [run-main-0] Started o.s.j.s.ServletContextHandler@52ea9e17{/jobs,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,051 [run-main-0] Started o.s.j.s.ServletContextHandler@3c619219{/jobs/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,052 [run-main-0] Started o.s.j.s.ServletContextHandler@4bb78001{/jobs/job,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,053 [run-main-0] Started o.s.j.s.ServletContextHandler@291881d8{/jobs/job/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,054 [run-main-0] Started o.s.j.s.ServletContextHandler@19592fb8{/stages,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,054 [run-main-0] Started o.s.j.s.ServletContextHandler@6e19031a{/stages/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,055 [run-main-0] Started o.s.j.s.ServletContextHandler@3369199c{/stages/stage,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,056 [run-main-0] Started o.s.j.s.ServletContextHandler@28415bf8{/stages/stage/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,057 [run-main-0] Started o.s.j.s.ServletContextHandler@539f0295{/stages/pool,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,057 [run-main-0] Started o.s.j.s.ServletContextHandler@1f02d186{/stages/pool/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,058 [run-main-0] Started o.s.j.s.ServletContextHandler@2599037b{/storage,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,059 [run-main-0] Started o.s.j.s.ServletContextHandler@72512ab{/storage/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,059 [run-main-0] Started o.s.j.s.ServletContextHandler@2c052370{/storage/rdd,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,060 [run-main-0] Started o.s.j.s.ServletContextHandler@25824997{/storage/rdd/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,061 [run-main-0] Started o.s.j.s.ServletContextHandler@486d1487{/environment,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,061 [run-main-0] Started o.s.j.s.ServletContextHandler@6772e76b{/environment/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,062 [run-main-0] Started o.s.j.s.ServletContextHandler@62694656{/executors,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,063 [run-main-0] Started o.s.j.s.ServletContextHandler@5590657d{/executors/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,063 [run-main-0] Started o.s.j.s.ServletContextHandler@75799a64{/executors/threadDump,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,064 [run-main-0] Started o.s.j.s.ServletContextHandler@540721f{/executors/threadDump/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,070 [run-main-0] Started o.s.j.s.ServletContextHandler@2db3cf17{/static,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,071 [run-main-0] Started o.s.j.s.ServletContextHandler@7e5041aa{/,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,073 [run-main-0] Started o.s.j.s.ServletContextHandler@3a168dc2{/api,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,074 [run-main-0] Started o.s.j.s.ServletContextHandler@1c0d88d7{/jobs/job/kill,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,075 [run-main-0] Started o.s.j.s.ServletContextHandler@1a3b2b91{/stages/stage/kill,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,077 [run-main-0] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.2:4040
INFO : 2019-08-31 18:03:06,158 [run-main-0] Starting executor ID driver on host localhost
INFO : 2019-08-31 18:03:06,183 [run-main-0] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63770.
INFO : 2019-08-31 18:03:06,184 [run-main-0] Server created on 192.168.1.2:63770
INFO : 2019-08-31 18:03:06,185 [run-main-0] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
INFO : 2019-08-31 18:03:06,219 [run-main-0] Registering BlockManager BlockManagerId(driver, 192.168.1.2, 63770, None)
INFO : 2019-08-31 18:03:06,223 [dispatcher-event-loop-2] Registering block manager 192.168.1.2:63770 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.1.2, 63770, None)
INFO : 2019-08-31 18:03:06,228 [run-main-0] Registered BlockManager BlockManagerId(driver, 192.168.1.2, 63770, None)
INFO : 2019-08-31 18:03:06,229 [run-main-0] Initialized BlockManager: BlockManagerId(driver, 192.168.1.2, 63770, None)
INFO : 2019-08-31 18:03:06,399 [run-main-0] Started o.s.j.s.ServletContextHandler@5232da38{/metrics/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,579 [run-main-0] Logging events to file:/tmp/spark-events/local-1567288986135
INFO : 2019-08-31 18:03:06,646 [run-main-0] *** AreaOfInterestApp Spark session built. Press Ctrl C to stop.
INFO : 2019-08-31 18:03:06,660 [run-main-0] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/objektwerks/workspace/spark.area.of.interest/spark-warehouse/').
INFO : 2019-08-31 18:03:06,660 [run-main-0] Warehouse path is 'file:/Users/objektwerks/workspace/spark.area.of.interest/spark-warehouse/'.
INFO : 2019-08-31 18:03:06,669 [run-main-0] Started o.s.j.s.ServletContextHandler@16c53b98{/SQL,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,669 [run-main-0] Started o.s.j.s.ServletContextHandler@75b2cd60{/SQL/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,670 [run-main-0] Started o.s.j.s.ServletContextHandler@4c6f3367{/SQL/execution,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,670 [run-main-0] Started o.s.j.s.ServletContextHandler@7d1b4715{/SQL/execution/json,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:06,671 [run-main-0] Started o.s.j.s.ServletContextHandler@34caa327{/static/sql,null,AVAILABLE,@Spark}
INFO : 2019-08-31 18:03:07,084 [run-main-0] Registered StateStoreCoordinator endpoint
INFO : 2019-08-31 18:03:07,715 [run-main-0] Writing atomically to file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/metadata using temp file file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/.metadata.629dd389-fb7b-4ed5-bc28-dc8df8e48ffa.tmp
INFO : 2019-08-31 18:03:07,840 [run-main-0] Renamed temp file file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/.metadata.629dd389-fb7b-4ed5-bc28-dc8df8e48ffa.tmp to file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/metadata
INFO : 2019-08-31 18:03:07,875 [run-main-0] Starting [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]. Use file:///private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878 to store the query checkpoint.
INFO : 2019-08-31 18:03:08,181 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Set the compact interval to 10 [defaultCompactInterval: 10]
INFO : 2019-08-31 18:03:08,189 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] maxFilesPerBatch = None, maxFileAgeMs = 604800000
INFO : 2019-08-31 18:03:08,190 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Using Source [FileStreamSource[file:/Users/objektwerks/workspace/spark.area.of.interest/data/hits]] from DataSourceV1 named 'FileSource[./data/hits]' [DataSource(org.apache.spark.sql.SparkSession@57e652c7,csv,List(),Some(StructType(StructField(id,StringType,true), StructField(utc,LongType,false), StructField(latitude,DoubleType,false), StructField(longitude,DoubleType,false))),List(),None,Map(delimiter -> ,, header -> true, basePath -> ./data/hits, path -> ./data/hits),None)]
INFO : 2019-08-31 18:03:08,198 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Starting new streaming query.
INFO : 2019-08-31 18:03:08,204 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Stream started from {}
INFO : 2019-08-31 18:03:08,235 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Writing atomically to file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/sources/0/0 using temp file file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/sources/0/.0.e269ba1b-5823-46ba-8ec4-e053487a5c31.tmp
INFO : 2019-08-31 18:03:08,268 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Renamed temp file file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/sources/0/.0.e269ba1b-5823-46ba-8ec4-e053487a5c31.tmp to file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/sources/0/0
INFO : 2019-08-31 18:03:08,269 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Log offset set to 0 with 1 new files
INFO : 2019-08-31 18:03:08,290 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Writing atomically to file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/offsets/0 using temp file file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/offsets/.0.8987fa63-5c3a-4b8a-a78d-ff005d4a42aa.tmp
INFO : 2019-08-31 18:03:08,323 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Renamed temp file file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/offsets/.0.8987fa63-5c3a-4b8a-a78d-ff005d4a42aa.tmp to file:/private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878/offsets/0
INFO : 2019-08-31 18:03:08,324 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1567288988273,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
INFO : 2019-08-31 18:03:08,359 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Processing 1 files from 0:0
INFO : 2019-08-31 18:03:08,577 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Pruning directories with: 
INFO : 2019-08-31 18:03:08,579 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Post-Scan Filters: 
INFO : 2019-08-31 18:03:08,582 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Output Data Schema: struct<id: string, utc: bigint, latitude: double, longitude: double ... 2 more fields>
INFO : 2019-08-31 18:03:08,588 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Pushed Filters: 
INFO : 2019-08-31 18:03:08,926 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Code generated in 185.064042 ms
INFO : 2019-08-31 18:03:09,080 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Code generated in 57.78594 ms
INFO : 2019-08-31 18:03:09,151 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Block broadcast_0 stored as values in memory (estimated size 222.6 KB, free 2004.4 MB)
INFO : 2019-08-31 18:03:09,333 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.8 KB, free 2004.4 MB)
INFO : 2019-08-31 18:03:09,335 [dispatcher-event-loop-6] Added broadcast_0_piece0 in memory on 192.168.1.2:63770 (size: 20.8 KB, free: 2004.6 MB)
INFO : 2019-08-31 18:03:09,338 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Created broadcast 0 from start at AreaOfInterestApp.scala:83
INFO : 2019-08-31 18:03:09,363 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
INFO : 2019-08-31 18:03:09,467 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Start processing data source writer: org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@7a2c3e2d. The input RDD has 1 partitions.
INFO : 2019-08-31 18:03:09,470 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Starting job: start at AreaOfInterestApp.scala:83
INFO : 2019-08-31 18:03:09,483 [dag-scheduler-event-loop] Got job 0 (start at AreaOfInterestApp.scala:83) with 1 output partitions
INFO : 2019-08-31 18:03:09,483 [dag-scheduler-event-loop] Final stage: ResultStage 0 (start at AreaOfInterestApp.scala:83)
INFO : 2019-08-31 18:03:09,484 [dag-scheduler-event-loop] Parents of final stage: List()
INFO : 2019-08-31 18:03:09,485 [dag-scheduler-event-loop] Missing parents: List()
INFO : 2019-08-31 18:03:09,492 [dag-scheduler-event-loop] Submitting ResultStage 0 (MapPartitionsRDD[1] at start at AreaOfInterestApp.scala:83), which has no missing parents
INFO : 2019-08-31 18:03:09,554 [dag-scheduler-event-loop] Block broadcast_1 stored as values in memory (estimated size 29.7 KB, free 2004.3 MB)
INFO : 2019-08-31 18:03:09,572 [dag-scheduler-event-loop] Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.1 KB, free 2004.3 MB)
INFO : 2019-08-31 18:03:09,572 [dispatcher-event-loop-7] Added broadcast_1_piece0 in memory on 192.168.1.2:63770 (size: 12.1 KB, free: 2004.6 MB)
INFO : 2019-08-31 18:03:09,573 [dag-scheduler-event-loop] Created broadcast 1 from broadcast at DAGScheduler.scala:1161
INFO : 2019-08-31 18:03:09,586 [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at start at AreaOfInterestApp.scala:83) (first 15 tasks are for partitions Vector(0))
INFO : 2019-08-31 18:03:09,587 [dag-scheduler-event-loop] Adding task set 0.0 with 1 tasks
INFO : 2019-08-31 18:03:09,627 [dispatcher-event-loop-0] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8355 bytes)
INFO : 2019-08-31 18:03:09,642 [Executor task launch worker for task 0] Running task 0.0 in stage 0.0 (TID 0)
INFO : 2019-08-31 18:03:09,727 [Executor task launch worker for task 0] Reading File path: file:///Users/objektwerks/workspace/spark.area.of.interest/data/hits/hits.txt, range: 0-189, partition values: [empty row]
INFO : 2019-08-31 18:03:09,750 [Executor task launch worker for task 0] Code generated in 15.899279 ms
ERROR: 2019-08-31 18:03:09,811 [Executor task launch worker for task 0] Aborting task
java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution$lzycompute(Dataset.scala:3026)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution(Dataset.scala:3024)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2715)
	at aoi.AreaOfInterest$.mapHitToAreaOfInterests(AreaOfInterest.scala:31)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR: 2019-08-31 18:03:09,941 [Executor task launch worker for task 0] Aborting commit for partition 0 (task 0, attempt 0stage 0.0)
ERROR: 2019-08-31 18:03:09,942 [Executor task launch worker for task 0] Aborted commit for partition 0 (task 0, attempt 0stage 0.0)
ERROR: 2019-08-31 18:03:09,955 [Executor task launch worker for task 0] Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution$lzycompute(Dataset.scala:3026)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution(Dataset.scala:3024)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2715)
	at aoi.AreaOfInterest$.mapHitToAreaOfInterests(AreaOfInterest.scala:31)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
WARN : 2019-08-31 18:03:10,238 [task-result-getter-0] Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution$lzycompute(Dataset.scala:3026)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution(Dataset.scala:3024)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2715)
	at aoi.AreaOfInterest$.mapHitToAreaOfInterests(AreaOfInterest.scala:31)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

ERROR: 2019-08-31 18:03:10,246 [task-result-getter-0] Task 0 in stage 0.0 failed 1 times; aborting job
INFO : 2019-08-31 18:03:10,248 [task-result-getter-0] Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO : 2019-08-31 18:03:10,261 [dag-scheduler-event-loop] Cancelling stage 0
INFO : 2019-08-31 18:03:10,263 [dag-scheduler-event-loop] Killing all running tasks in stage 0: Stage cancelled
INFO : 2019-08-31 18:03:10,267 [dag-scheduler-event-loop] ResultStage 0 (start at AreaOfInterestApp.scala:83) failed in 0.751 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution$lzycompute(Dataset.scala:3026)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution(Dataset.scala:3024)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2715)
	at aoi.AreaOfInterest$.mapHitToAreaOfInterests(AreaOfInterest.scala:31)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
INFO : 2019-08-31 18:03:10,276 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Job 0 failed: start at AreaOfInterestApp.scala:83, took 0.805739 s
ERROR: 2019-08-31 18:03:10,278 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Data source writer org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@7a2c3e2d is aborting.
ERROR: 2019-08-31 18:03:10,280 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Data source writer org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@7a2c3e2d aborted.
ERROR: 2019-08-31 18:03:10,294 [stream execution thread for [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33]] Query [id = 6e98acf3-946a-4754-b671-f92903ddb0e7, runId = aeec715b-513b-497a-aee3-0c7e76fece33] terminated with error
org.apache.spark.SparkException: Writing job aborted.
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2782)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2782)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2782)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$17.apply(MicroBatchExecution.scala:540)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:535)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:534)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution$lzycompute(Dataset.scala:3026)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution(Dataset.scala:3024)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2715)
	at aoi.AreaOfInterest$.mapHitToAreaOfInterests(AreaOfInterest.scala:31)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:64)
	... 35 more
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution$lzycompute(Dataset.scala:3026)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$rddQueryExecution(Dataset.scala:3024)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2715)
	at aoi.AreaOfInterest$.mapHitToAreaOfInterests(AreaOfInterest.scala:31)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at aoi.AreaOfInterestApp$$anonfun$6.apply(AreaOfInterestApp.scala:77)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR: 2019-08-31 18:03:10,822 [spark-listener-group-streams] uncaught error in thread spark-listener-group-streams, stopping SparkContext
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
ERROR: 2019-08-31 18:03:10,826 [Spark Context Cleaner] Error in cleaning thread
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:181)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:178)
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:73)
ERROR: 2019-08-31 18:03:10,825 [spark-listener-group-appStatus] uncaught error in thread spark-listener-group-appStatus, stopping SparkContext
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
ERROR: 2019-08-31 18:03:10,824 [spark-listener-group-shared] uncaught error in thread spark-listener-group-shared, stopping SparkContext
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
ERROR: 2019-08-31 18:03:10,824 [spark-listener-group-eventLog] uncaught error in thread spark-listener-group-eventLog, stopping SparkContext
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
WARN : 2019-08-31 18:03:10,823 [org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner] exception in the cleaner thread but it will continue to run
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:2989)
	at java.lang.Thread.run(Thread.java:748)
ERROR: 2019-08-31 18:03:10,823 [spark-listener-group-executorManagement] uncaught error in thread spark-listener-group-executorManagement, stopping SparkContext
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
INFO : 2019-08-31 18:03:10,943 [stop-spark-context] SparkContext already stopped.
ERROR: 2019-08-31 18:03:10,943 [spark-listener-group-eventLog] throw uncaught fatal error in thread spark-listener-group-eventLog
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
INFO : 2019-08-31 18:03:10,940 [stop-spark-context] SparkContext already stopped.
ERROR: 2019-08-31 18:03:10,940 [spark-listener-group-shared] throw uncaught fatal error in thread spark-listener-group-shared
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
INFO : 2019-08-31 18:03:10,938 [stop-spark-context] SparkContext already stopped.
ERROR: 2019-08-31 18:03:10,938 [spark-listener-group-appStatus] throw uncaught fatal error in thread spark-listener-group-appStatus
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
INFO : 2019-08-31 18:03:10,892 [stop-spark-context] Stopped Spark@7abc100c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
ERROR: 2019-08-31 18:03:10,884 [spark-listener-group-streams] throw uncaught fatal error in thread spark-listener-group-streams
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
INFO : 2019-08-31 18:03:10,945 [stop-spark-context] SparkContext already stopped.
ERROR: 2019-08-31 18:03:10,945 [spark-listener-group-executorManagement] throw uncaught fatal error in thread spark-listener-group-executorManagement
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
INFO : 2019-08-31 18:03:10,965 [stop-spark-context] Stopped Spark web UI at http://192.168.1.2:4040
INFO : 2019-08-31 18:03:10,967 [shutdownHook1] SparkContext already stopped.
INFO : 2019-08-31 18:03:10,967 [shutdownHook1] *** AreaOfInterestApp Spark session stopped.
INFO : 2019-08-31 18:03:10,970 [Thread-3] Shutdown hook called
INFO : 2019-08-31 18:03:10,992 [Thread-3] Shutdown hook called
INFO : 2019-08-31 18:03:10,993 [Thread-3] Deleting directory /private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/temporary-751e54fb-15ca-4941-90dc-bd2ae79ea878
INFO : 2019-08-31 18:03:11,004 [Thread-3] Deleting directory /private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/spark-08892cdc-5843-43a3-ad2f-6d24f9d269eb/userFiles-69eef364-389a-4da8-86d4-0292297ab496
INFO : 2019-08-31 18:03:11,013 [Thread-3] Deleting directory /private/var/folders/cd/1wkmn_gn2d164ch7c66b_46h0000gn/T/spark-08892cdc-5843-43a3-ad2f-6d24f9d269eb
